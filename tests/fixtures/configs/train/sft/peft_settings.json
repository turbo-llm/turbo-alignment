"peft_settings": {
            "r": 8,
            "lora_alpha": 16,
            "lora_dropout": 0.05,
            "target_modules": [
                "q_proj",
                "v_proj"
            ],
            "task_type": "CAUSAL_LM",
            "modules_to_save": ["embed_tokens", "lm_head"],
            "name": "LORA"
        },