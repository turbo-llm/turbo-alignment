from typing import Optional, Tuple

import torch
from torch import nn
from deepspeed.sequence.layer import DistributedAttention  # , _SeqAllToAll
import torch.distributed as dist
from transformers.cache_utils import Cache
from transformers.models.gemma2.modeling_gemma2 import (
    Gemma2Config,
    Gemma2Attention,
    repeat_kv,
    Gemma2RotaryEmbedding,
    apply_rotary_pos_emb,
)
from transformers.utils import (
    is_flash_attn_2_available,
    is_flash_attn_greater_or_equal,
    is_flash_attn_greater_or_equal_2_10,
    logging,
)
from turbo_alignment.modeling.parallel_states import (
    get_sequence_parallel_group,
    get_sequence_parallel_world_size,
    sequence_parallel_is_enabled,
    get_sequence_parallel_rank,
)
# from turbo_alignment.modeling.gemma2.verbose_attn import DistributedAttention

if is_flash_attn_2_available():
    from transformers.modeling_flash_attention_utils import _flash_attention_forward


logger = logging.get_logger(__name__)


class _SeqAllToAll(torch.autograd.Function):
    @staticmethod
    def forward(ctx: 'Any', group: dist.ProcessGroup, input: 'Tensor', scatter_idx: int, gather_idx: int) -> 'Tensor':
        ctx.group = group
        ctx.scatter_idx = scatter_idx
        ctx.gather_idx = gather_idx

        import deepspeed.comm as dist
        seq_world_size = dist.get_world_size(group)

        input_list = [t.contiguous() for t in torch.tensor_split(input, seq_world_size, scatter_idx)]
        output_list = [torch.empty_like(input_list[0]) for _ in range(seq_world_size)]
        # TODO Use all_to_all_single instead
        dist.all_to_all(output_list, input_list, group=group)
        return torch.cat(output_list, dim=gather_idx).contiguous()

    @staticmethod
    def backward(ctx: 'Any', *grad_output: 'Tensor') -> Tuple[None, 'Tensor', None, None]:
        return (None, _SeqAllToAll.apply(ctx.group, *grad_output, ctx.gather_idx, ctx.scatter_idx), None, None)


class Gemma2AttentionUlysses(torch.nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: 'Gemma2Config', layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        if layer_idx is None:
            logger.warning_once(
                f"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will "
                "lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` "
                "when creating this class."
            )

        self.attention_dropout = config.attention_dropout
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = config.head_dim
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.max_position_embeddings = config.max_position_embeddings
        self.rope_theta = config.rope_theta
        self.is_causal = True
        self.scaling = config.query_pre_attn_scalar**-0.5

        if self.hidden_size % self.num_heads != 0:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )

        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)
        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)
        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)
        self.sliding_window = config.sliding_window if not bool(layer_idx % 2) else None
        self.rotary_emb = Gemma2RotaryEmbedding(
            self.head_dim,
            max_position_embeddings=self.max_position_embeddings,
            base=self.rope_theta,
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        bsz, q_len, _ = hidden_states.size()
        seq_len = q_len * get_sequence_parallel_world_size()

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        if sequence_parallel_is_enabled():
            filename = f'/mnt/p.geyn/key_weight_{dist.get_rank()}.npy'
        else:
            filename = f'/mnt/p.geyn/key_weight.npy'

        import os
        if not os.path.exists(filename):
            self.k_proj.weight.detach().float().cpu().numpy().tofile(filename)

        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        # print(f'{dist.get_rank()=} {position_ids=}')

        # if sequence_parallel_is_enabled():
        #     filename = f'/mnt/p.geyn/query_states_before_rank_{dist.get_rank()}.npy'
        # else:
        #     filename = f'/mnt/p.geyn/query_states_before.npy'

        # import os
        # if not os.path.exists(filename):
        #     query_states.detach().float().cpu().numpy().tofile(filename)

        # if sequence_parallel_is_enabled():
        #     filename = f'/mnt/p.geyn/key_states_before_rank_{dist.get_rank()}.npy'
        # else:
        #     filename = f'/mnt/p.geyn/key_states_before.npy'

        # import os
        # if not os.path.exists(filename):
        #     key_states.detach().float().cpu().numpy().tofile(filename)

        cos, sin = self.rotary_emb(value_states, position_ids)

        if sequence_parallel_is_enabled():
            sp_world_size = get_sequence_parallel_world_size()
            chunk_size = seq_len // sp_world_size
            start = chunk_size * get_sequence_parallel_rank()
            end = chunk_size * (get_sequence_parallel_rank() + 1)
            cos = cos[:, start:end]
            sin = sin[:, start:end]

        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_value is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {
                "sin": sin,
                "cos": cos,
                "sliding_window": self.sliding_window,
                "cache_position": cache_position,
            }
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        scatter_idx = 1
        gather_idx = 2

        spg = get_sequence_parallel_group()

        query_states = _SeqAllToAll().apply(spg, query_states, scatter_idx, gather_idx)
        key_states = _SeqAllToAll().apply(spg, key_states, scatter_idx, gather_idx)
        value_states = _SeqAllToAll().apply(spg, value_states, scatter_idx, gather_idx)

        # if sequence_parallel_is_enabled():
        #     filename = f'/mnt/p.geyn/query_states_rank_{dist.get_rank()}.npy'
        # else:
        #     filename = f'/mnt/p.geyn/query_states.npy'

        # import os
        # if not os.path.exists(filename):
        #     query_states.detach().float().cpu().numpy().tofile(filename)

        # if sequence_parallel_is_enabled():
        #     filename = f'/mnt/p.geyn/keys_states_rank_{dist.get_rank()}.npy'
        # else:
        #     filename = f'/mnt/p.geyn/keys_states.npy'

        # if not os.path.exists(filename):
        #     key_states.detach().float().cpu().numpy().tofile(filename)

        # print(f'{query_states.size()=} {key_states.size()=}')

        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scaling

        if self.config.attn_logit_softcapping is not None:
            attn_weights = attn_weights / self.config.attn_logit_softcapping
            attn_weights = torch.tanh(attn_weights)
            attn_weights = attn_weights * self.config.attn_logit_softcapping
        if attention_mask is not None:  # no matter the length, we just slice it
            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
            # print(f'{attn_weights.size()=} {key_states.size()=} {causal_mask.size()=} {attention_mask.size()=} {key_states.size()=}')
            attn_weights = attn_weights + causal_mask

        # upcast attention to fp32
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)
        attn_output = torch.matmul(attn_weights, value_states)

        if attn_output.size() != (bsz, self.num_heads // get_sequence_parallel_world_size(), seq_len, self.head_dim):
            raise ValueError(
                f"`attn_output` should be of size {(bsz, self.num_heads // get_sequence_parallel_world_size(), seq_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )

        attn_output = _SeqAllToAll().apply(spg, attn_output, gather_idx, scatter_idx)
        attn_output = attn_output.transpose(1, 2).contiguous()

        attn_output = attn_output.view(bsz, q_len, -1)
        attn_output = self.o_proj(attn_output)

        if not output_attentions:
            attn_weights = None

        return attn_output, attn_weights, past_key_value


class Gemma2FlashAttention2Ulysses(Gemma2Attention):
    """
    Gemma2 flash attention module with DeepSpeed Ulysses. This module inherits from `Gemma2Attention` as the weights of the module stays
    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of
    flash attention and deal with padding tokens in case the input contains any of them.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.
        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.
        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).
        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.LongTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        output_attentions = False

        bsz, q_len, _ = hidden_states.size()

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        # Flash attention requires the input to have the shape
        # batch_size x seq_length x head_dim x hidden_dim
        # therefore we just need to keep the original shape
        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        cos, sin = self.rotary_emb(value_states, position_ids)

        if sequence_parallel_is_enabled():
            sp_world_size = get_sequence_parallel_world_size()
            seq_len = cos.size(1)
            chunk_size = seq_len // sp_world_size
            start = chunk_size * get_sequence_parallel_rank()
            end = chunk_size * (get_sequence_parallel_rank() + 1)
            cos = cos[:, start:end]
            sin = sin[:, start:end]

        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_value is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {
                "sin": sin,
                "cos": cos,
                "sliding_window": self.sliding_window,
                "cache_position": cache_position,
            }
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        if attention_mask is not None:
            seq_len = attention_mask.shape[1]
            key_states = key_states[:, :, :seq_len]
            value_states = value_states[:, :, :seq_len]

        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache
        # to be able to avoid many of these transpose/reshape/view.
        query_states = query_states.transpose(1, 2)
        key_states = key_states.transpose(1, 2)
        value_states = value_states.transpose(1, 2)

        dropout_rate = self.attention_dropout if self.training else 0.0

        # In PEFT, usually we cast the layer norms in float32 for training stability reasons
        # therefore the input hidden states gets silently casted in float32. Hence, we need
        # cast them back in the correct dtype just to be sure everything works as expected.
        # This might slowdown training & inference so it is recommended to not cast the LayerNorms
        # in fp32. (Gemma2RMSNorm handles it correctly)

        input_dtype = query_states.dtype
        if input_dtype == torch.float32:
            if torch.is_autocast_enabled():
                target_dtype = torch.get_autocast_gpu_dtype()
            # Handle the case where the model is quantized
            elif hasattr(self.config, "_pre_quantization_dtype"):
                target_dtype = self.config._pre_quantization_dtype
            else:
                target_dtype = self.q_proj.weight.dtype

            logger.warning_once(
                f"The input hidden states seems to be silently casted in float32, this might be related to"
                f" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in"
                f" {target_dtype}."
            )

            query_states = query_states.to(target_dtype)
            key_states = key_states.to(target_dtype)
            value_states = value_states.to(target_dtype)

        logger.debug(f'Before attention class: {dist.get_rank()=} {query_states.size()=} {key_states.size()=} {value_states.size()=}')
        _forward = DistributedAttention(
            _flash_attention_forward,
            get_sequence_parallel_group(),
            scatter_idx=2,
            gather_idx=1,
        )

        # in old version of deepspeed, DistributedAttention doest not support kwargs, so be careful with argument order
        flash_q_len = q_len if attention_mask is None else attention_mask.size(1)
        cap = self.config.attn_logit_softcapping if is_flash_attn_greater_or_equal("2.6.0") else None
        attn_output = _forward(
            query_states,
            key_states,
            value_states,
            attention_mask,
            flash_q_len,
            self.is_causal,
            dropout_rate,
            None,  # position_ids
            self.scaling,
            self.sliding_window,
            self._flash_attn_uses_top_left_mask,
            cap,  # softcap
        )

        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()
        attn_output = self.o_proj(attn_output)

        if not output_attentions:
            attn_weights = None

        return attn_output, attn_weights, past_key_value
