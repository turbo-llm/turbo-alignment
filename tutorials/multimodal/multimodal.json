{
    "train_dataset_settings": {
      "sources": [
        {
          "name": "train",
          "records_path": "train_chat.jsonl",
          "num_samples": 10
        }
      ],
      "prompt_template": {
        "role_tag_mapping": {
          "bot": "<bot>",
          "user": "<user>",
          "system": "<system>"
        },
        "prefix_template": "<RS>{role}",
        "suffix_template": "</RS>"
      },
      "modality_token_mapping": {
        "image": "<img>",
        "audio": "<audio>"
      },
      "modality_reader_settings_mapping": {
        "image": {
          "reader_type": "clip",
          "reader_path": "openai/clip-vit-base-patch32"
        },
        "audio": null
      },
      "n_modality_embeddings": 49,
      "start_modality_token": "<MS>",
      "end_modality_token": "</MS>",
      "dataset_type": "multimodal",
      "max_tokens_count": 2000,
      "only_answer_loss": true,
      "truncate_top": false
    },
    "val_dataset_settings": {
      "sources": [
        {
          "name": "test",
          "records_path": "val_chat.jsonl",
          "num_samples": 10
        }
      ],
      "prompt_template": {
        "role_tag_mapping": {
          "bot": "<bot>",
          "user": "<user>",
          "system": "<system>"
        },
        "prefix_template": "<RS>{role}",
        "suffix_template": "</RS>"
      },
      "modality_token_mapping": {
        "image": "<img>",
        "audio": "<audio>"
      },
      "modality_reader_settings_mapping": {
        "image": {
          "reader_type": "clip",
          "reader_path": "openai/clip-vit-base-patch32"
        },
        "audio": null
      },
      "n_modality_embeddings": 49,
      "start_modality_token": "<MS>",
      "end_modality_token": "</MS>",
      "dataset_type": "multimodal",
      "max_tokens_count": 2000,
      "only_answer_loss": true,
      "truncate_top": false
    },
    "model_settings": {
      "model_path": "Qwen/Qwen1.5-0.5B",
      "model_type": "causal",
      "transformers_settings": {},
      "peft_settings": {
        "r": 16,
        "lora_alpha": 16,
        "lora_dropout": 0.05,
        "target_modules": [
          "q_proj",
          "k_proj"
        ],
        "task_type": "CAUSAL_LM",
        "modules_to_save": [
          "embed_tokens",
          "lm_head"
        ],
        "name": "LORA"
      },
      "embeddings_initialization_strategy": {
        "<RS>": "bot",
        "</RS>": "bot",
        "<MS>": "bot",
        "</MS>": "bot",
        "<bot>": "bot",
        "<user>": "bot",
        "<system>": "bot",
        "<img>": "bot",
        "<audio>": "bot"
      }
    },
    "tokenizer_settings": {},
    "special_tokens_settings": {
      "bos_token": "<s>",
      "eos_token": "</s>"
      },
    "trainer_settings": {
      "evaluation_strategy": "steps",
      "save_strategy": "steps",
      "per_device_train_batch_size": 1,
      "per_device_eval_batch_size": 1,
      "gradient_accumulation_steps": 1,
      "logging_steps": 1,
      "eval_steps": 32,
      "save_steps": 32,
      "learning_rate": 1e-6,
      "num_train_epochs": 1,
      "lr_scheduler_type": "cosine",
      "warmup_steps": 0,
      "fp16": false,
      "bf16": false,
      "optim": "adamw_torch",
      "save_total_limit": 1
    },
    "logging_settings": {
      "project_name": "alignment",
      "run_name": "multimodal",
      "entity": "turbo-alignment"
    },
    "modality_encoder_settings_mapping": {
      "image": {
        "modality_encoder_type": "clip",
        "is_pickle": false,
        "encoder_path": "openai/clip-vit-base-patch32"
      },
      "audio": null
    },
    "modality_projector_mapping": {
      "image": "llava",
      "audio": null
    },
    "modality_projector_initialization_mapping": {
      "image": null,
      "audio": null
    },
    "cherry_pick_settings": {
      "generator_transformers_settings": {
        "num_beams": 1,
        "max_new_tokens": 128,
        "repetition_penalty": 1.1,
        "do_sample": true,
        "stop_strings": "</RS>"
      },
      "custom_generation_settings": {
        "skip_special_tokens": true
      },
      "dataset_settings": {
        "sources": [
          {
            "name": "cherry_picks",
            "records_path": "val_chat.jsonl",
            "num_samples": 10
          }
        ],
        "prompt_template": {
          "role_tag_mapping": {
            "bot": "<bot>",
            "user": "<user>",
            "system": "<system>"
          },
          "prefix_template": "<RS>{role}",
          "suffix_template": "</RS>"
        },
        "dataset_type": "multimodal",
        "max_tokens_count": 2000,
        "n_modality_embeddings": 49,
        "start_modality_token": "<MS>",
        "end_modality_token": "</MS>",
        "only_answer_loss": true,
        "modality_token_mapping": {
          "image": "<img>",
          "audio": "<audio>"
        },
        "modality_reader_settings_mapping": {
          "image": {
            "reader_type": "clip",
            "reader_path": "openai/clip-vit-base-patch32"
          },
          "audio": null
        },
        "truncate_top": false,
        "random_cut": true
      },
      "metric_settings": []
    },
    "log_path": "tutorial_multimodal_output"
  }
